{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 지금까지 배운 내용을 기초로 tensorflow의 mnist example을\n",
    "# CNN으로 구현해 보아요\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Data loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# 전처리 단계 (결측치, 이상치, 정규화, feature engineering)\n",
    "# mnist예제에서는 이미 전처리가 끝난 상태라서 따로 할게 없다!\n",
    "\n",
    "# Model 정의\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 1. placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "dropout_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# 버전에 따라 1.5 => keep_prob = 0.8  살리는 비율\n",
    "#                  rate = 0.2 죽이는 비율\n",
    "\n",
    "# 2. Convolution Layer\n",
    "x_img = tf.reshape(X, [-1,28,28,1])\n",
    "L1 = tf.layers.conv2d(inputs=x_img, filters=32,\n",
    "                         kernel_size=[3,3], padding=\"SAME\",\n",
    "                         strides=1, activation=tf.nn.relu)\n",
    "\n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2],\n",
    "                                padding=\"SAME\",\n",
    "                                strides=2)\n",
    "\n",
    "L1 = tf.layers.dropout(inputs=L1, rate=dropout_rate)\n",
    "\n",
    "# W1 = tf.Variable(tf.random_normal([3,3,1,32])) # filter\n",
    "# L1 = tf.nn.conv2d(x_img, W1, strides=[1,1,1,1], # CNN(합성곱)\n",
    "#                              padding=\"SAME\")\n",
    "# L1 = tf.nn.relu(L1)                              # relu\n",
    "# L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "#                                            padding=\"SAME\")\n",
    "\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
    "                         kernel_size=[3,3],  padding=\"SAME\",\n",
    "                         strides=1, activation=tf.nn.relu)\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],\n",
    "                                padding=\"SAME\",\n",
    "                                strides=2)\n",
    "\n",
    "L2 = tf.layers.dropout(inputs=L2, rate=dropout_rate)\n",
    "\n",
    "## W2 = tf.Variable(tf.random_normal([3,3,32,64]))\n",
    "## L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1],\n",
    "##                              padding=\"SAME\")\n",
    "## L2 = tf.nn.relu(L2)\n",
    "## L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
    "##                                            padding=\"SAME\")\n",
    "\n",
    "\n",
    "# 3. FC Layer (= Dense Layer) \n",
    "# Dense Layer 로 바꾸자\n",
    "\n",
    "L2 = tf.reshape(L2,[-1, 7*7*64])\n",
    "\n",
    "Dense1 = tf.layers.dense(inputs=L2, units=256,\n",
    "                            activation=tf.nn.relu) # 밑에 3줄을 한번에 처리\n",
    "Dense1 = tf.layers.dropout(inputs=Dense1, rate=dropout_rate)\n",
    "\n",
    "# W3 = tf.get_variable(\"weight3\", shape=[7*7*64, 256],\n",
    "#                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b3 = tf.Variable(tf.random_normal([256]), name=\"bias3\")\n",
    "# _layer1 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "# layer1 = tf.nn.dropout(_layer1, keep_prob=dropout_rate)\n",
    "\n",
    "Dense2 = tf.layers.dense(inputs=Dense1, units=128,\n",
    "                            activation=tf.nn.relu) # 밑에 3줄을 한번에 처리\n",
    "Dense2 = tf.layers.dropout(inputs=Dense2, rate=dropout_rate)\n",
    "\n",
    "# W4 = tf.get_variable(\"weight4\", shape=[256,512],\n",
    "#                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b4 = tf.Variable(tf.random_normal([512]), name=\"bias4\")\n",
    "# _layer2 = tf.nn.relu(tf.matmul(layer1, W4)+b4)\n",
    "# layer2 = tf.nn.dropout(_layer2, keep_prob=dropout_rate)\n",
    "\n",
    "Dense3 = tf.layers.dense(inputs=Dense2, units=512,\n",
    "                            activation=tf.nn.relu) # 밑에 3줄을 한번에 처리\n",
    "Dense3 = tf.layers.dropout(inputs=Dense3, rate=dropout_rate)\n",
    "\n",
    "H = tf.layers.dense(inputs=Dense3, units=10) # H를 한번에 만들어 버린다\n",
    "\n",
    "# W5 = tf.get_variable(\"weight5\", shape=[512,10],\n",
    "#                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "# b5 = tf.Variable(tf.random_normal([10]), name=\"bias5\")\n",
    "\n",
    "# # H\n",
    "# logit = tf.matmul(layer2, W5)+b5\n",
    "# H = tf.nn.softmax(logit)\n",
    "\n",
    "# cost function\n",
    "cost = tf.losses.softmax_cross_entropy(Y, H) # 순서가 바뀐다\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "#                                                                    labels = Y))\n",
    "\n",
    "#\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_4/BiasAdd:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값 : 0.22042438387870789\n",
      "Cost값 : 0.2108023762702942\n",
      "Cost값 : 0.21726827323436737\n",
      "Cost값 : 0.02184995450079441\n",
      "Cost값 : 0.09577726572751999\n",
      "Cost값 : 0.1975776106119156\n",
      "Cost값 : 0.056538380682468414\n",
      "Cost값 : 0.18843795359134674\n",
      "Cost값 : 0.046463608741760254\n",
      "Cost값 : 0.08279932290315628\n",
      "정확도는 : 0.9664999842643738\n",
      "Wall time: 21min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 학습\n",
    "train_epoch = 30\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/ batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], \n",
    "                               feed_dict={ X : batch_x,\n",
    "                                            Y : batch_y})\n",
    "    if step % 3 == 0:\n",
    "        print(\"Cost값 : {}\".format(cost_val))\n",
    "\n",
    "        \n",
    "# 학습이 종료 되었으니 정확도를 측정\n",
    "\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "print(\"정확도는 : {}\".format(sess.run(accuracy, feed_dict={X : mnist.test.images,\n",
    "                                                          Y : mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############결국 우리의 MNIST 예제는 multinomial 예제다!\n",
    "############# 이미지 1개에 대한 예측값이 \n",
    "############# H의 도출값은 예) [0.5,0.3,0.2,0.001,0.99,0.44,....]\n",
    "\n",
    "### ensenble은 모델을 여러개 만든다.(10개의 모델)\n",
    "### 이미지 1개에 대한 각 모델의 예측값\n",
    "### H1 -> [0.5,0.3,0.2,0.001,0.99,0.44,....]\n",
    "### H2 -> [0.4,0.2,0.3,0.011,0.90,0.64]\n",
    "### H3 -> [0.8,0.3,0.4,0.005,0.78,,,,,]\n",
    "### .\n",
    "### .\n",
    "### .\n",
    "### .\n",
    "### Hn\n",
    "\n",
    "### Sum=> [1.7,0.8,0.9,0.017,......]\n",
    "### 한사람의 전문가한테 의견을 구하는게 아니라\n",
    "### 여러사람의 전문가에게 의견을 구해서 가장 그럴듯한 대답으로 찾아내는 방식\n",
    "\n",
    "### 프로그래밍 영역\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle MNIST deeplearning\n",
    "\n",
    "# 필요한 module import\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Data Loading\n",
    "\n",
    "k_train_mnist = pd.read_csv(\"./data/digit-recognizer/train.csv\")\n",
    "k_test_mnist = pd.read_csv(\"./data/digit-recognizer/test.csv\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(k_train_mnist)\n",
    "\n",
    "x_data = scaler.fit_transform(k_train_mnist.iloc[:,1:])\n",
    "y_data = pd.get_dummies(k_train_mnist[\"label\"]).values\n",
    "\n",
    "pre_x_data = scaler.fit_transform(k_test_mnist.values)\n",
    "\n",
    "\n",
    "# train / test 구분\n",
    "train_num = int(x_data.shape[0] * 0.8)\n",
    "\n",
    "# train data set\n",
    "train_x_data = x_data[:train_num]\n",
    "train_y_data = y_data[:train_num]\n",
    "\n",
    "# test data set\n",
    "test_x_data = x_data[train_num:]\n",
    "test_y_data = y_data[train_num:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수로 모델 1개 만드는 로직 짜기\n",
    "\n",
    "\n",
    "result = []\n",
    "def make_H(X,Y,d_rate):\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(shape=[None, 784], dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "    d_rate = tf.placeholder(dtype=tf.float32)\n",
    "    \n",
    "    x_img = tf.reshape(X, [-1,28,28,1])\n",
    "    L1 = tf.layers.conv2d(inputs=x_img, filters=32,\n",
    "                             kernel_size=[3,3], strides=1,\n",
    "                             padding=\"SAME\", activation=tf.nn.relu)\n",
    "    L1 = tf.layers.max_pooling2d(inputs=L1, pool_size = [2,2],\n",
    "                                   padding=\"SAME\", strides=2)\n",
    "    L1 = tf.layers.dropout(inputs=L1, rate=d_rate)\n",
    "    \n",
    "    L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
    "                             kernel_size=[3,3], strides=1,\n",
    "                             padding=\"SAME\", activation=tf.nn.relu)\n",
    "    L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2],\n",
    "                                    padding=\"SAME\", strides=2)\n",
    "    L2 = tf.layers.dropout(inputs=L2, rate=d_rate)\n",
    "    \n",
    "    Dl3 = tf.reshape(L2, [-1, 7*7*64])\n",
    "    \n",
    "    Dl3 = tf.layers.dense(inputs=Dl3, units=256, activation=tf.nn.relu)\n",
    "    \n",
    "    Dl4 = tf.layers.dense(inputs=Dl3, units=128, activation=tf.nn.relu)\n",
    "    \n",
    "    Dl5 = tf.layers.dense(inputs=Dl4, units=512, activation=tf.nn.relu)\n",
    "    \n",
    "    H = tf.layers.dense(inputs=Dl5, units=10)\n",
    "    \n",
    "    cost = tf.losses.softmax_cross_entropy(Y, H) # 순서가 바뀐다\n",
    "\n",
    "    train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_epoch = 30\n",
    "    batch_size = 100\n",
    "\n",
    "    for step in range(train_epoch):\n",
    "        num_of_iter = int(train_x_data.shape[0]/batch_size)\n",
    "        cost_val = 0\n",
    "        for i in range(num_of_iter):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            cut_train_x = train_x_data[start:end]\n",
    "            cut_train_y = train_y_data[start:end]\n",
    "            _, cost_val = sess.run([train, cost], \n",
    "                                   feed_dict={ X : cut_train_x,\n",
    "                                                Y : cut_train_y,\n",
    "                                                 d_rate : 0.8})\n",
    "\n",
    "        if step % 3 == 0:\n",
    "            print(\"Cost값 : {}\".format(cost_val))\n",
    "\n",
    "    \n",
    "        tmp = sess.run(H, feed_dict={X : test_x_data})\n",
    "        result = result + tmp\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값 : 0.25770843029022217\n",
      "Cost값 : 0.020579295232892036\n",
      "Cost값 : 0.06677761673927307\n",
      "Cost값 : 0.01443882379680872\n",
      "Cost값 : 0.12112454324960709\n",
      "Cost값 : 0.17724022269248962\n",
      "Cost값 : 0.04085906967520714\n",
      "Cost값 : 0.10043641179800034\n",
      "Cost값 : 0.016445619985461235\n",
      "Cost값 : 0.042957957834005356\n",
      "[[7.6954727, -5.101029, -4.3286467, -2.8076994, -1.2830647, 2.3543415, 5.3073344, -3.5426598, 2.308227, -2.9392533], [16.94471, -14.081453, -2.9537275, -3.6962588, -2.9622986, -1.5341485, -1.0021567, -8.313989, 1.1892447, -4.5225606], [29.236557, -32.527355, -10.678991, -12.057344, -10.8120165, -15.958943, -23.468891, -16.069109, -8.816397, -17.166168], [20.418243, -22.493408, 0.76354086, 0.84865487, -11.092806, -15.209392, -1.5207148, -15.477078, -4.1384335, -15.488857], [22.097359, -26.415403, -3.7350209, -7.209144, -20.795237, -16.879614, -4.900503, -12.226725, -4.1675777, -17.12745], [19.178059, -22.028955, -10.316296, -10.046954, -9.583377, -16.447462, -1.2524173, -6.1927648, -1.0661488, -14.020364], [18.088766, -19.591747, -1.3187279, -8.844463, -15.629663, -15.495896, -4.569765, -10.092185, 2.4925349, -14.23318], [34.647057, -33.142635, -9.3510475, -25.041143, -25.527515, -26.781157, -14.49179, -20.63257, -11.448673, -27.081743], [34.011925, -25.052267, -18.443855, -21.445139, -40.21954, -23.124178, 7.5663385, -7.637181, 4.8932295, -14.999121], [26.617315, -24.86119, -18.200876, -16.14942, -29.217688, -24.584429, 7.0880985, -12.104751, -2.1028175, -13.440504], [23.94264, -30.59022, -15.016155, -21.136805, -33.09866, -42.001255, -2.1156986, -21.54545, -2.903943, -9.594789], [37.935955, -32.355907, -36.95668, -48.474255, -45.816544, -61.261944, -18.422337, -29.829855, -11.638488, -7.3506303], [18.102484, -38.144917, -1.0718951, -34.930874, -23.280127, -28.549948, 3.0274072, -36.458176, -12.792515, -13.384867], [10.880315, -35.66317, -18.553247, -51.4825, -27.796492, -37.625443, -30.135427, -42.985966, -18.40193, -29.616253], [51.971954, -76.05469, -49.61364, -137.19286, -86.61752, -92.46734, -59.523228, -89.85415, -23.15461, -72.41975], [46.87727, -35.369366, -32.304066, -45.234646, -16.062386, -51.15417, 13.25794, -36.704685, -7.0875425, -3.6117125], [40.53602, -54.255898, -18.615282, -49.132027, -14.275762, -51.27344, 3.6518192, -67.53684, -19.562662, -27.522518], [36.487774, -47.720833, -3.6011074, -32.45886, -25.542715, -42.01146, 4.2439523, -30.972982, 8.605951, -8.811331], [41.501457, -43.85449, -16.357038, -70.916695, -16.164429, -47.945694, 15.215517, -50.792343, 0.9625175, -3.5217626], [25.770657, -16.490122, -6.6276484, -30.428017, -3.9140484, -27.330519, 1.493626, -23.292648, 8.585496, -4.340303], [10.149241, -2.6631248, 0.26891956, -17.724243, -0.44032413, -14.664777, 6.371718, -14.727407, 4.5033956, 2.1187313], [8.025951, -12.627149, -4.2883816, -9.276203, -5.7271914, -4.118926, 6.2848277, -13.902005, 3.525269, -0.5758062], [75.10096, -33.237713, 2.9595253, -54.874706, -5.1813755, -29.056559, 2.2119162, -56.308342, 6.620298, 2.1876364], [49.91064, -6.7997646, -1.5310686, -50.07858, -3.6607487, -26.359308, 5.113758, -32.03897, 13.985835, 3.4087799], [48.886578, -12.91309, 13.352195, -55.46254, -5.1454706, -30.103594, 7.419941, -35.79427, 16.877314, 6.4393454], [49.10094, -19.829517, 11.4088125, -57.24553, -8.445301, -33.19155, 14.464628, -37.140743, 16.052624, 1.591152], [60.514725, -73.42137, 41.884396, -103.14671, -8.645829, -62.778496, 11.544589, -72.2995, 32.463055, 41.891926], [60.733974, -81.51205, 23.940018, -74.94767, -6.0590625, -29.627274, 4.7960496, -55.802185, -0.6639412, 12.738499], [37.3494, -42.677433, 16.908928, -61.47095, -6.3062634, -15.204321, 29.564375, -45.280254, 3.9880686, -9.588237], [73.982796, -91.05438, 14.680846, -117.493744, -14.001103, -31.353418, 35.98146, -50.557022, 18.763006, -14.187406]]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    a=make_H(train_x_data, train_y_data, 0.2)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1036.697193145752, -1012.5306437015533, -157.69621840119362, -1229.557319521904, -523.3045591711998, -911.7403128147125, 23.20236837863922, -956.1128082275391, 17.87038677930832, -275.16849333047867]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_typedict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-c4c1073a01ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mnew_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y_data\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m print(\"정확도는 : {}\".format(sess.run(accuracy, feed_dict={X : test_x_data,\n\u001b[0;32m     25\u001b[0m                                                           \u001b[0mY\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtest_y_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '_typedict' object is not callable"
     ]
    }
   ],
   "source": [
    "a[0]\n",
    "b=[0,0,0,0,0,0,0,0,0,0]\n",
    "for j in range(len(a)):\n",
    "    for i in range(len(a[0])):\n",
    "        b[i] += a[j][i]\n",
    "print(b)\n",
    "b = np.array(b)\n",
    "b\n",
    "# sess.run(b, feed_dict={X : test_x_data})\n",
    "\n",
    "\n",
    "# b=tf.layers.dense(inputs=b, units=10)\n",
    "# sess.run(b, feed_dict={ X : test_x_data})\n",
    "# print(H, b)\n",
    "                   \n",
    "b=np.reshape(b,[1,10])\n",
    "b\n",
    "#Y.shape\n",
    "new_predict = np.argmax(b, 1)\n",
    "new_predict\n",
    "new_y = np.argmax(test_y_data , 1)\n",
    "correct = np.equal(new_predict, new_y)\n",
    "accuracy = np.mean(np.cast(correct))\n",
    "print(\"정확도는 : {}\".format(sess.run(accuracy, feed_dict={X : test_x_data,\n",
    "                                                          Y : test_y_data,\n",
    "                                                              dropout_rate : 1})))\n",
    "# np.argmax(a, axis=None, out=None)\n",
    "# tf.argmax(\n",
    "#     input,\n",
    "#     axis=None,\n",
    "#     name=None,\n",
    "#     dimension=None,\n",
    "#     output_type=tf.int64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 학습시켜보기! \n",
    "# batch function\n",
    "train_epoch = 10\n",
    "batch_size = 100\n",
    "\n",
    "x = \n",
    "\n",
    "for i in range(10):\n",
    "    make_H()\n",
    "    \n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(train_x_data.shape[0]/batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        cut_train_x = train_x_data[start:end]\n",
    "        cut_train_y = train_y_data[start:end]\n",
    "        _, cost_val = sess.run([train, cost], \n",
    "                               feed_dict={ X : cut_train_x,\n",
    "                                            Y : cut_train_y,\n",
    "                                             drop_rate : 0.8})\n",
    "    \n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(\"Cost값 : {}\".format(cost_val))\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
