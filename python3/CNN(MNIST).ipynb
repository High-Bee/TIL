{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN ( Convolutional Neural Network ) : 합성곱 신경망\n",
    "\n",
    "## FC Layer ( Fully Connected Layer )\n",
    "> 이전 layer의 모든 node가 다음 layer의 모든 node에 연결되어\n",
    "> 학습되는 layer구조 (신경망의 노드가 연결되 여러 layer를 이루는 구조를 일컫는다)\n",
    "> Fc Layer를 다른말로 Dense Layer라고 한다!\n",
    "     \n",
    "### 지금까지 우리가 작업한 신경망은 모두 FC Layer를 이용하고 있다.\n",
    "\n",
    "### FC Layer의 특징은 MNIST의 예제처럼 입력데이터가 1차원으로 한정된다.\n",
    "> 즉 각각의 이미지가 1차원으로 표현이 되어야 한다.\n",
    "> 그래서 2차원 이미지를 우리가 1차원으로 변환시켜서 사용한 것이다.\n",
    "\n",
    "### 우리가 사용한 MNIST예제는 상당히 가단한 이미지 학습, 예측 예제.\n",
    "\n",
    "### 이미지 학습의 가장 큰 문제는\n",
    "### 이미지가 살짝 휘어있거나, 크기가 제 각각이거나, 변형이 조금만 생겨도\n",
    "### 학습이 너무 힘들어진다.\n",
    "\n",
    "### 이런 경우에는 Training data가 굉장히 많이 필요하고\n",
    "### 추가적으로 학습할 때 많은 시간을 요구하게 된다.\n",
    "\n",
    "### 방법론 연구의 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법론 연구의 시작\n",
    "\n",
    "### 사람이 학습하는 방식을 모델링했다.\n",
    "### 이미지의 픽셀값을 그대로 입력하는게 아니라\n",
    "### 이미지를 대표하는 특징들을 도출해서 신경망에 여러개\n",
    "### 넣어서 학습하는 방식이다!\n",
    "\n",
    "### 1장이 컬러사진은  width, height, color(depth) 3차원으로 표현\n",
    "### 총 입력에 여러장이 사진이 사용되기 때문에 입력데이터는 4차원으로 표현\n",
    "\n",
    "### 실제 이미지 한장은 3차원이고 이놈을 flatten시켜서 1차원으로 표현해야 한다.\n",
    "### 크기를 조절해야 되기 때문에 공간에 대한 데이터를 유실할 우려가 있다.\n",
    "### 이런 데이터 유실때문에 학습과 예측에 문제가 발생하게 된다.\n",
    "\n",
    "### 공간데이터의 유실을 없애고 이미지의 특성을 추출해서 학습이 용이하게 \n",
    "### 만드는 방식 => CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight의 shape : (2, 2, 1, 3)\n",
      "conv2d의 shape : (1, 2, 2, 3)\n",
      "[[[[ 12. 120. -12.]\n",
      "   [ 16. 160. -16.]]\n",
      "\n",
      "  [[ 24. 240. -24.]\n",
      "   [ 28. 280. -28.]]]]\n",
      "pool의 shape : (1, 2, 2, 3)\n",
      "[[[[ 28. 280. -12.]\n",
      "   [ 28. 280. -16.]]\n",
      "\n",
      "  [[ 28. 280. -24.]\n",
      "   [ 28. 280. -28.]]]]\n"
     ]
    }
   ],
   "source": [
    "# Code로 알아보자!\n",
    "# 사용되어지는 함수부터 알아보자!\n",
    "# Sample CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 입력데이터 형식 : 3*3*1\n",
    "image = np.arange(1,10,dtype=np.float32).reshape(1,3,3,1)\n",
    "image\n",
    "\n",
    "# Activation map을 위한 filter를 정의(형태) : (width, height, color, filter개수)\n",
    "# filter (2,2,1,3)\n",
    "weight = np.array([[[[1,10,-1]],[[1,10,-1]]],[[[1,10,-1]],[[1,10,-1]]]], dtype=np.float32)\n",
    "print(\"weight의 shape : {}\".format(weight.shape)) # weight의 shape : (2, 2, 1, 3)\n",
    "\n",
    "# stride = 1 (가로, 세로 1씩 움직이자)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1,1,1,1], padding=\"VALID\") # VALID : 패딩하지않는다(사이즈 줄어든다.)\n",
    "                                                                         # SAME : 패딩한다 (사이즈 유지)\n",
    "print(\"conv2d의 shape : {}\".format(conv2d.shape)) # conv2d의 shape : (1, 2, 2, 3)\n",
    "\n",
    "sess = tf.Session()\n",
    "conv2d = sess.run(conv2d)\n",
    "print(conv2d)\n",
    "# pooling layer\n",
    "pool = tf.nn.max_pool(conv2d, ksize=[1,2,2,1], strides=[1,1,1,1], padding=\"SAME\")\n",
    "print(\"pool의 shape : {}\".format(pool.shape))\n",
    "pool = sess.run(pool)\n",
    "print(pool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "(1, 28, 28, 1)\n",
      "conv2d의 shape : (1, 14, 14, 5)\n",
      "pool의 shape : (5, 14, 14, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1424a532c18>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convolution 결과 이미지가 원본이미지에 비해 어떻게 다른지 \n",
    "# 눈으로 확인해 보자!\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "   \n",
    "# data loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "img = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "\n",
    "# 해당 이미지를 Convolution 처리를 해보자!\n",
    "# 입력데이터 => (이미지 개수, width, height, color) => 1,3,3,1\n",
    "img = img.reshape(-1,28,28,1)\n",
    "print(img.shape)\n",
    "\n",
    "# Activation map을 위한 filter를 정의(형태) : (width, height, color, filter개수)\n",
    "W = tf.Variable(tf.random_normal([3,3,1,5]), name=\"filter1\")\n",
    "\n",
    "conv2d = tf.nn.conv2d(img, W, strides=[1,2,2,1], padding=\"SAME\")\n",
    "print(\"conv2d의 shape : {}\".format(conv2d.shape)) # (1, 14, 14, 5)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d = sess.run(conv2d)\n",
    "\n",
    "\n",
    "# 이미지를 표현하기 위해서 축을 전환\n",
    "# (1,14,14,5) => (5,14,14,1)\n",
    "conv2d_img = np.swapaxes(conv2d,0,3)\n",
    "conv2d_img.shape\n",
    "plt.imshow(conv2d_img[4].reshape(14,14), cmap=\"Greys\")\n",
    "\n",
    "pool = tf.nn.max_pool(conv2d_img, ksize=[1,2,2,1], strides=[1,1,1,1], padding=\"SAME\")\n",
    "print(\"pool의 shape : {}\".format(pool.shape))\n",
    "pool = sess.run(pool)\n",
    "pool_img = pool\n",
    "pool_img.shape\n",
    "plt.imshow(pool_img[2].reshape(14,14), cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# tensorflow-MNIST with CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# Graph 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# Placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Convolution Layer (Layer1)\n",
    "x_img = tf.reshape(X, [-1,28,28,1]) # 몇장인지 모르고 28,28이고 색은 그레이스케일 1\n",
    "W1 = tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "L1 = tf.nn.conv2d(x_img, W1, strides=[1,1,1,1], padding=\"SAME\") # strides와 padding은 parameter값\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "# Layer2\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64]))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding=\"SAME\")\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "# 이렇게 만든 data를 FC Layer에 넣어서 학습!\n",
    "L2 = tf.reshape(L2, [-1, 7*7*64])\n",
    "#L2.shape # TensorShape([Dimension(None), Dimension(7), Dimension(7), Dimension(64)])\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape=[7*7*64,256],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([256]), name=\"bias3\")\n",
    "_L3 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "L3 = tf.nn.dropout(_L3, keep_prob=drop_rate)\n",
    "\n",
    "# Layer 4\n",
    "W4 = tf.get_variable(\"weight4\", shape=[256,256],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([256]), name=\"bias4\")\n",
    "_L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "L4 = tf.nn.dropout(_L4, keep_prob=drop_rate)\n",
    "\n",
    "# Layer 5\n",
    "W5 = tf.get_variable(\"weight5\", shape=[256,10],\n",
    "                        initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]), name=\"bias5\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(L4, W5)+b5\n",
    "H = tf.nn.relu(logit)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                    labels = Y))\n",
    "\n",
    "# train\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.005).minimize(cost)\n",
    "\n",
    "# Session & reset\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost : 0.6264487504959106\n",
      "Cost : 0.06371894478797913\n",
      "Cost : 0.10392849147319794\n",
      "Cost : 0.024695362895727158\n",
      "Cost : 0.05510765686631203\n",
      "Cost : 0.10045552998781204\n",
      "Cost : 0.008368580602109432\n",
      "Cost : 0.05328496918082237\n",
      "Cost : 0.07345348596572876\n",
      "Cost : 0.21184097230434418\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 학습\n",
    "num_of_epoch = 50\n",
    "batch_size = 100\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _, cost_val = sess.run([train, cost], feed_dict = { X : batch_x,\n",
    "                                                              Y : batch_y,\n",
    "                                                              drop_rate : 0.75})\n",
    "    if step % 5 == 0:\n",
    "        print(\"Cost : {}\".format(cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9830999970436096\n"
     ]
    }
   ],
   "source": [
    "# accuracy\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "print(\"Accuracy : {}\".format(sess.run(accuracy, feed_dict = { X : mnist.test.images,\n",
    "                                                                Y : mnist.test.labels,\n",
    "                                                                drop_rate : 1.0})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[CPU_ENV]",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
